{
  "paragraphs": [
    {
      "text": "%md\n### Initialize Spark Context and Spark SQL Context",
      "dateUpdated": "Oct 5, 2016 1:59:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955391_1539050677",
      "id": "20161005-135915_111932721",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eInitialize Spark Context and Spark SQL Context\u003c/h3\u003e\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql import SQLContext\nsc\n",
      "dateUpdated": "Nov 12, 2016 11:19:51 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955391_1539050677",
      "id": "20161005-135915_1340486389",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:19:51 AM",
      "dateFinished": "Nov 12, 2016 11:19:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Creating DataFrames from JSON files",
      "dateUpdated": "Oct 5, 2016 1:59:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955392_1524814968",
      "id": "20161005-135915_1948290243",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eCreating DataFrames from JSON files\u003c/h3\u003e\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nusers \u003d sqlContext.read.json(\"/zeppelin/datasets/yelp/yelp_academic_dataset_user.json\")\nusers.registerTempTable(\"userTable\")",
      "dateUpdated": "Nov 12, 2016 11:19:54 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955392_1524814968",
      "id": "20161005-135915_462807227",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:19:54 AM",
      "dateFinished": "Nov 12, 2016 11:19:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect review_count, count(*) as count from userTable group by review_count",
      "dateUpdated": "Nov 12, 2016 11:20:00 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "review_count",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "review_count",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955392_1524814968",
      "id": "20161005-135915_1112444037",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: Table not found: userTable; line 1 pos 44\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:315)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:300)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)\n\tat org.apache.spark.sql.DataFrame.\u003cinit\u003e(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:295)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:00 AM",
      "dateFinished": "Nov 12, 2016 11:20:00 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nusers.printSchema()",
      "dateUpdated": "Nov 12, 2016 11:20:08 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955398_1524045470",
      "id": "20161005-135915_1019309987",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "root\n |-- average_stars: double (nullable \u003d true)\n |-- compliments: struct (nullable \u003d true)\n |    |-- cool: long (nullable \u003d true)\n |    |-- cute: long (nullable \u003d true)\n |    |-- funny: long (nullable \u003d true)\n |    |-- hot: long (nullable \u003d true)\n |    |-- list: long (nullable \u003d true)\n |    |-- more: long (nullable \u003d true)\n |    |-- note: long (nullable \u003d true)\n |    |-- photos: long (nullable \u003d true)\n |    |-- plain: long (nullable \u003d true)\n |    |-- profile: long (nullable \u003d true)\n |    |-- writer: long (nullable \u003d true)\n |-- elite: array (nullable \u003d true)\n |    |-- element: long (containsNull \u003d true)\n |-- fans: long (nullable \u003d true)\n |-- friends: array (nullable \u003d true)\n |    |-- element: string (containsNull \u003d true)\n |-- name: string (nullable \u003d true)\n |-- review_count: long (nullable \u003d true)\n |-- type: string (nullable \u003d true)\n |-- user_id: string (nullable \u003d true)\n |-- votes: struct (nullable \u003d true)\n |    |-- cool: long (nullable \u003d true)\n |    |-- funny: long (nullable \u003d true)\n |    |-- useful: long (nullable \u003d true)\n |-- yelping_since: string (nullable \u003d true)\n\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:08 AM",
      "dateFinished": "Nov 12, 2016 11:20:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Prepare data input for KMeans\n",
      "dateUpdated": "Oct 5, 2016 1:59:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955400_1521736976",
      "id": "20161005-135915_1291299436",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003ePrepare data input for KMeans\u003c/h3\u003e\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.mllib.linalg import Vectors\n\n# Youtube video k-means tutorial https://www.youtube.com/watch?v\u003d_aWzGGNrcic\n# Coursera video tutorial : https://www.coursera.org/learn/machine-learning/home/week/8\n\nuserReviews \u003d users.map(lambda user : Row(user.user_id, user.name, user.average_stars, Vectors.dense([user.review_count])))\nuserReviewDF \u003d sqlContext.createDataFrame(userReviews, [\"user_id\", \"name\", \"average_stars\", \"features\"])\nuserReviewDF.cache()\nuserReviewDF.show(5)",
      "dateUpdated": "Nov 12, 2016 11:20:12 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955401_1521352227",
      "id": "20161005-135915_2018712433",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+---------+-------------+--------+\n|             user_id|     name|average_stars|features|\n+--------------------+---------+-------------+--------+\n|18kPq7GPye-YQ3LyK...|   Russel|         4.14| [108.0]|\n|rpOyqD_893cqmDAtJ...|   Jeremy|         3.67|[1292.0]|\n|4U9kSBLuBDU391x6b...|  Michael|         3.68| [395.0]|\n|fHtTaujcyKvXglE33...|      Ken|         4.64|  [11.0]|\n|SIBCL7HBkrP4llolm...|Katherine|          3.8|  [66.0]|\n+--------------------+---------+-------------+--------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:12 AM",
      "dateFinished": "Nov 12, 2016 11:20:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Setup KMeans model\n",
      "dateUpdated": "Oct 5, 2016 1:59:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955401_1521352227",
      "id": "20161005-135915_1313835610",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eSetup KMeans model\u003c/h3\u003e\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.ml.clustering import KMeans\n\n# official docs : http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.clustering\nkmeans \u003d KMeans(k\u003d10, initMode\u003d\"random\")\nmodel \u003d kmeans.fit(userReviewDF)\n\n\n",
      "dateUpdated": "Nov 12, 2016 11:20:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955402_1522506474",
      "id": "20161005-135915_2034522183",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:20 AM",
      "dateFinished": "Nov 12, 2016 11:20:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ncenters \u003d model.clusterCenters()\nprint centers\nresults \u003d model.transform(userReviewDF)\nresults.show(5)\nresults.registerTempTable(\"results\")",
      "dateUpdated": "Nov 12, 2016 11:20:30 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955403_1522121725",
      "id": "20161005-135915_143735241",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[array([ 2.80558537]), array([ 558.26439578]), array([ 233.19459379]), array([ 0.]), array([ 94.34529598]), array([ 37.85773385]), array([ 16.13547282]), array([ 7.01181854]), array([ 1551.46153846]), array([ 1.])]\n+--------------------+---------+-------------+--------+----------+\n|             user_id|     name|average_stars|features|prediction|\n+--------------------+---------+-------------+--------+----------+\n|18kPq7GPye-YQ3LyK...|   Russel|         4.14| [108.0]|         4|\n|rpOyqD_893cqmDAtJ...|   Jeremy|         3.67|[1292.0]|         8|\n|4U9kSBLuBDU391x6b...|  Michael|         3.68| [395.0]|         2|\n|fHtTaujcyKvXglE33...|      Ken|         4.64|  [11.0]|         7|\n|SIBCL7HBkrP4llolm...|Katherine|          3.8|  [66.0]|         5|\n+--------------------+---------+-------------+--------+----------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:30 AM",
      "dateFinished": "Nov 12, 2016 11:20:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom math import sqrt\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center \u003d centers[point[\"prediction\"]]\n    return sqrt(sum([x**2 for x in (point[\"features\"] - center)]))",
      "dateUpdated": "Nov 12, 2016 11:20:39 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955404_1520197981",
      "id": "20161005-135915_1752769337",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:39 AM",
      "dateFinished": "Nov 12, 2016 11:20:39 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nerrors \u003d results.map(lambda point : error(point)).reduce(lambda a,b : a + b)\nprint errors",
      "dateUpdated": "Nov 12, 2016 11:20:50 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955404_1520197981",
      "id": "20161005-135915_1524001078",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "1439162.53381\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:20:50 AM",
      "dateFinished": "Nov 12, 2016 11:20:57 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nsqlContext.sql(\"select prediction, avg(average_stars) as avg_star from results group by prediction\").show()\nsqlContext.sql(\"select * from results where prediction \u003d 0\").show()",
      "dateUpdated": "Nov 12, 2016 11:21:00 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955405_1519813232",
      "id": "20161005-135915_17536075",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----------+------------------+\n|prediction|          avg_star|\n+----------+------------------+\n|         0|3.7154331258050264|\n|         1| 3.753024085637825|\n|         2| 3.767149547803619|\n|         3|               2.5|\n|         4|3.7758655648321704|\n|         5| 3.785892898937419|\n|         6|3.7950972877358495|\n|         7| 3.772825412416008|\n|         8| 3.749460317460318|\n|         9|3.6756718735999523|\n+----------+------------------+\n\n+--------------------+--------+-------------+--------+----------+\n|             user_id|    name|average_stars|features|prediction|\n+--------------------+--------+-------------+--------+----------+\n|5OlCB4cJ3CUksr3ON...| Jeffrey|         1.33|   [3.0]|         0|\n|xOQVHYN1roRZKpLvA...|    Chad|          2.5|   [2.0]|         0|\n|Yt4bcTwDMDCGSD2eQ...|    Jess|         3.67|   [3.0]|         0|\n|9VjrmRRwAkiuEvt8U...|   Chris|          1.0|   [3.0]|         0|\n|0bS4Xdg6xyXKepmay...|   Steve|          3.0|   [3.0]|         0|\n|3vgNYwAbVFsU0SfkI...|    Kyle|          4.0|   [3.0]|         0|\n|AeUdBbzKRSy0G_SQ7...|    Tony|          1.0|   [2.0]|         0|\n|U0RkOYGP0OzOkjn6a...|Sunshine|          4.5|   [2.0]|         0|\n|Po0R9Sj1xJcBS_9a6...|   Tracy|         2.33|   [3.0]|         0|\n|GP68togo6XkoHHBE6...| Breanne|          3.0|   [2.0]|         0|\n|uxJTQh7JXYRx-Vrs1...|   Kelly|         2.67|   [3.0]|         0|\n|N8sTFiMM0J4kK2Q_6...|  Connie|          1.5|   [2.0]|         0|\n|z-0l9wtrlGBSyMlj4...|Lawrence|          3.5|   [3.0]|         0|\n|NSHoWa7SjbWVwZveI...|    Juan|          5.0|   [2.0]|         0|\n|pex4UYLrNXELoafCd...| Bethany|          5.0|   [4.0]|         0|\n|WbeUV41fKMLJTKFSv...|     Dan|         4.67|   [3.0]|         0|\n|rrGaPRXG4O8jNhbyT...|    Adam|          2.0|   [2.0]|         0|\n|wd5LbxpbBEP7FVgU_...|   Raven|          5.0|   [3.0]|         0|\n|hSYqAqxdifcULKX_-...|      JC|          2.0|   [2.0]|         0|\n|rLOlKKUULGTkjfPna...|     Gin|         4.67|   [3.0]|         0|\n+--------------------+--------+-------------+--------+----------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "dateStarted": "Nov 12, 2016 11:21:00 AM",
      "dateFinished": "Nov 12, 2016 11:21:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Oct 5, 2016 1:59:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475675955405_1519813232",
      "id": "20161005-135915_14002898",
      "dateCreated": "Oct 5, 2016 1:59:15 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "SparkMLPythonDevTalksParticipants",
  "id": "2BX275WGU",
  "angularObjects": {
    "2C3WZUK33": [],
    "2C1YA5D2F": [],
    "2C2S9E6XK": [],
    "2C381K1GS": [],
    "2BZPA1EZS": [],
    "2BZV245P9": [],
    "2C43YS5P2": [],
    "2BZTQ77CG": [],
    "2C1UP5V1X": [],
    "2C1D5ZAWQ": [],
    "2C2P2YH1D": [],
    "2BZTHUGNC": [],
    "2C1WZ6MNJ": [],
    "2C1FTY57N": [],
    "2C1N9QYZB": []
  },
  "config": {},
  "info": {}
}